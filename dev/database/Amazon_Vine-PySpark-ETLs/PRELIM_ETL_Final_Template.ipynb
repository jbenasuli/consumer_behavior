{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PRELIM_ETL-Final_Template",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.12.3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbenasuli/final_project/blob/database/dev/database/Amazon_Vine-PySpark-ETLs/PRELIM_ETL_Final_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V58rxea0HqSa"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 2.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.1.1'\n",
        "spark_version = 'spark-3.1.1'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xKwTpATHqSe"
      },
      "source": [
        "# Download the Postgres driver that will allow Spark to interact with Postgres.\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMqDAjVS0KN9"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"BigData-Challenge\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbbRSWPwJw4k"
      },
      "source": [
        "import datetime\n",
        "from pyspark.sql.functions import to_date\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StructField, StringType, IntegerType, StructType, BooleanType, DateType\n",
        "from pyspark import SparkFiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyBsySGuY-9V"
      },
      "source": [
        "### Load Amazon Data into Spark DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHY6-k4WgZRO"
      },
      "source": [
        "Note: Enter URL for Desired Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtCmBhQJY-9Z"
      },
      "source": [
        "#1 load product segment\n",
        "url = \"<database_url>\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.option(\"encoding\", \"UTF-8\").csv(SparkFiles.get(\"\"), sep=\"\\t\", header=True, inferSchema=True)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yUSe55VY-9t"
      },
      "source": [
        "### Create DataFrame - Perform Preliminary Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8REmY1aY-9u"
      },
      "source": [
        "#1 Check the schema, print row & column count\n",
        "df.printSchema()\n",
        "print((df.count(), len(df.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGosPEeAKCFm"
      },
      "source": [
        "#2 Drop the round 1 columns\n",
        "columns_to_drop = ['marketplace', 'product_parent', 'vine', 'review_headline', 'review_body', 'review_date']\n",
        "df_dropped = df.drop(*columns_to_drop)\n",
        "df_dropped.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cWyyOQyKB8v"
      },
      "source": [
        "#3 Filter Step 1:\n",
        "df_filtered = df_dropped.filter(df_dropped.verified_purchase == 'Y')\n",
        "df_filtered.show\n",
        "print((df_filtered.count(), len(df_filtered.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVJDJ4LBRHCV"
      },
      "source": [
        "#4 drop filtered verified_purchase column\n",
        "columns_to_drop = ['verified_purchase']\n",
        "df_dropped_2 = df_filtered.drop(*columns_to_drop)\n",
        "df_dropped_2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si3TGL5tHP1Z"
      },
      "source": [
        "## Create Analysis-Specific DFs/Tables\n",
        "### Perform Analysis-Specific Transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy5OVX7YHaXR"
      },
      "source": [
        "### Segmentation Analysis DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PldvjkUWHCt6"
      },
      "source": [
        "# 1 Create Segmentation DF by Droppig Addtional Columns\n",
        "segmentation_cols_drop = ['review_id', 'product_id', 'product_title', 'star_rating', 'helpful_votes', 'total_votes']\n",
        "segmentation_dropped_df = df_dropped_2.drop(*segmentation_cols_drop)\n",
        "segmentation_dropped_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWLZJ8LIWpbK"
      },
      "source": [
        "Note: must change Category Label name in withColumnRenamed('count(product_category)', 'name')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB7H2fA_HCnX"
      },
      "source": [
        "#2 Segmentation GroupBy\n",
        "#2a GroupBy customer_id\n",
        "#2b Count product_category and rename count columns as Segment Name\n",
        "segment_df = segmentation_dropped_df.groupby(\"customer_id\")\\\n",
        ".agg({'product_category':'count'}).withColumnRenamed('count(product_category)', 'name')\n",
        "#2c Check results\n",
        "segment_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4y9GGlYL46X"
      },
      "source": [
        "#3 Check segment_df Schema and Row Count\n",
        "segment_df.printSchema()\n",
        "print(segment_df.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C32fEqE9XE6y"
      },
      "source": [
        "Note: Column Name in df.sort('name' ...) must align with Column name from step 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnyAXUQ0OVTM"
      },
      "source": [
        "#4 Filter for Top n Results\n",
        "#4a Declare number of rows to filter by (100,000)\n",
        "row_count = 100000\n",
        "#4a Sort by Segment Desc and limit to row_count\n",
        "filtered_segment_df = segment_df.sort(\"<name>\", ascending=False).limit(row_count)\n",
        "#4b Check Results\n",
        "filtered_segment_df.show()\n",
        "print(filtered_segment_df.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuQnSLoxVvJ5"
      },
      "source": [
        "### Segmentation ETL Complete - Add Database Export Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1a7WsMM4Fkp"
      },
      "source": [
        "# Configure settings for RDS Postgres DB\n",
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://<db_connection_string>\"\n",
        "config = {\"user\": <username>, \n",
        "          \"password\"1: <password>, \n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOE5O-RDXg7y"
      },
      "source": [
        "Note: table name in table='name_segment' must align with table name in Postgres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvdvijJe4FiE"
      },
      "source": [
        "# Write segment table to Postgres/RDS\n",
        "# xx mins\n",
        "filtered_segment_df.write.jdbc(url=jdbc_url, table='<name_segment>', mode=mode, properties=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf1b-3fhV2vd"
      },
      "source": [
        "## Apriori Analysis DF\n",
        "### Enter Apriori Transfomations Below - Use df_dropped_2 as Start Point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5fqsDHoWRRo"
      },
      "source": [
        "# 1 Create Apriori DF by First Dropping Addtional Columns\n",
        "apriori_cols_drop = ['product_category', 'product_title', 'star_rating', 'helpful_votes', 'total_votes']\n",
        "apriori_dropped_df = df_dropped_2.drop(*apriori_cols_drop)\n",
        "apriori_dropped_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efnKvbn_YDxY"
      },
      "source": [
        "### Prelim Apriori ETL Complete - Add Database Export Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSTKJzcGO4wv"
      },
      "source": [
        "# Configure settings for RDS\n",
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://<db_connection_string>\"\n",
        "config = {\"user\":<username>, \n",
        "          \"password\": <password>, \n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmwVVxzUYIsn"
      },
      "source": [
        "Note: table name in table='segment_apriori' must align with table name in Postgres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwcIfmxwO33L"
      },
      "source": [
        "# Write segment_apriori table to RDS\n",
        "# 5 mins\n",
        "apriori_dropped_df.write.jdbc(url=jdbc_url, table='<segment>_apriori', mode=mode, properties=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H-EhptPWT_O"
      },
      "source": [
        "## Amazon Reviews S3 -> Postgres/RDS ETL Complete\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLAhPy7UY-6E"
      },
      "source": [
        "### Run Queries in Postgres to Confirm Segment ETL\n",
        "Check Row Count of Segment Table - SELECT COUNT(*) FROM name_segment;\n",
        "\n",
        "Check 10 Rows of Segment Table - SELECT * FROM name_segment LIMIT(10);\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA4ZFLCuY7bz"
      },
      "source": [
        "# Upon Confirmation of Above Checks Run This Cell\n",
        "print('Segment ETL Successful')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVw7yd0EZ0Cc"
      },
      "source": [
        "### Run Queries in Postgres to Confirm Apriori ETL\n",
        "Check Row Count of Apriori Table - SELECT COUNT(*) FROM segment_apriori;\n",
        "\n",
        "Check 10 Rows of Apriori Table - SELECT * FROM segment_apriori LIMIT(10);"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHJg88FDaN3p"
      },
      "source": [
        "# Upon Confirmation of Above Checks Run This Cell\n",
        "print('Apriori ETL Successful')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}